"use strict";

var _interopRequireDefault = require("@babel/runtime/helpers/interopRequireDefault");
Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.HarEntryValidationError = exports.HarEntries = void 0;
exports.getHarEntriesFromFs = getHarEntriesFromFs;
var _fs = _interopRequireDefault(require("fs"));
var _path = _interopRequireDefault(require("path"));
var _promises = _interopRequireDefault(require("node:fs/promises"));
var _stream = require("stream");
var _tsInvariant = _interopRequireDefault(require("ts-invariant"));
var _Pick = require("stream-json/filters/Pick");
var _StreamArray = require("stream-json/streamers/StreamArray");
var _Disassembler = require("stream-json/Disassembler");
var _Stringer = require("stream-json/Stringer");
var _streamChain = require("stream-chain");
var _harSchema = _interopRequireDefault(require("har-schema"));
var _ajv = _interopRequireDefault(require("ajv"));
var _ajvFormats = _interopRequireDefault(require("ajv-formats"));
var _isUrl = _interopRequireDefault(require("is-url"));
var _tsResults = require("ts-results");
var _nodeZlib = _interopRequireDefault(require("node:zlib"));
// replace with  stream.compose once it stabilises

async function* getHarEntriesFromFs(harPath) {
  const isDir = _fs.default.lstatSync(harPath).isDirectory();
  const harPaths = isDir ? await _promises.default.readdir(harPath).then(paths => paths.filter(p => _path.default.extname(p).toLowerCase() === '.har').map(p => _path.default.join(harPath, p))) : [harPath];
  for (const harPath of harPaths) {
    const harEntries = HarEntries.fromReadable(_fs.default.createReadStream(harPath));
    yield* harEntries;
  }
}
class HarEntries {
  static async *fromReadable(source) {
    (0, _tsInvariant.default)(!source.readableObjectMode, 'Expecting raw bytes to parse har entries');
    const parseEntries = (0, _Pick.withParser)({
      filter: 'log.entries',
      pathSeparator: '.'
    });
    const streamEntries = (0, _StreamArray.streamArray)();
    const rawEntries = (0, _streamChain.chain)([source, parseEntries, streamEntries]);
    const ajv = new _ajv.default({
      allErrors: true,
      strict: false,
      schemas: [...Object.values(_harSchema.default).map(rawSchema => {
        let schema = rawSchema;
        let {
          $schema,
          ...rest
        } = schema;
        return {
          ...rest
        };
      })]
    });
    const validator = (0, _ajvFormats.default)(ajv, {
      mode: 'fast'
    });
    const validate = validator.getSchema('entry.json');
    let entryCount = 0;
    try {
      for await (let {
        value
      } of rawEntries) {
        entryCount++;
        coerceBracketedIpAddress(value); // workaround Chrome writing IPv6 addresses with brackets around them (parsed from URI)
        if (validate(value)) {
          yield (0, _tsResults.Ok)(value);
        } else {
          // TODO: yield a Result, so we can propagate this error rather than deciding on skip here
          let validationError = new HarEntryValidationError(value, entryCount, validate.errors);
          yield (0, _tsResults.Err)(validationError);
        }
      }
    } catch (err) {
      if (err instanceof Error && err.message.includes('Parser')) {
        // duck typing, but it's the best we've got
        throw new Error(`Source could not be read as HAR: ${err.message}`);
      } else {
        throw err;
      }
    }
  }
  static async *fromProxyInteractions(interactions) {
    for await (let interaction of interactions) {
      var _interaction$request$;
      let httpVersion = interaction.request.httpVersion || '1.1';
      let requestBodyBuffer = interaction.request.body.buffer;
      let requestContentType = interaction.request.headers['content-type'];
      let queryString = [];
      try {
        const url = new URL(interaction.request.url);
        for (const [name, value] of url.searchParams) {
          queryString.push({
            name,
            value
          });
        }
      } catch (e) {
        continue;
      }
      const requestBodyEncoded = requestContentType && (await toBase64(requestBodyBuffer, interaction.response.headers['content-encoding']));
      let request = {
        method: interaction.request.method,
        url: interaction.request.url,
        httpVersion,
        headers: interaction.request.rawHeaders.map(([key, value]) => ({
          name: key,
          value
        })),
        headersSize: -1,
        // hard to calculate
        bodySize: interaction.request.body.buffer.length,
        cookies: [],
        // not available from proxy, but could parse from header
        queryString,
        postData: requestContentType && requestBodyEncoded ? {
          mimeType: requestContentType,
          text: requestBodyEncoded,
          encoding: 'base64',
          params: []
        } : undefined
      };
      let responseBodyBuffer = interaction.response.body.buffer;
      const responseBodyText = await toBase64(responseBodyBuffer, interaction.response.headers['content-encoding']);
      let response = {
        status: interaction.response.statusCode,
        statusText: interaction.response.statusMessage,
        httpVersion,
        headers: interaction.request.rawHeaders.map(([key, value]) => ({
          name: key,
          value
        })),
        headersSize: -1,
        // hard to calculate
        cookies: [],
        // not available from proxy, but could parse from header
        redirectURL: ((_interaction$request$ = interaction.request.rawHeaders.find(([key]) => key.toLowerCase() === 'location')) === null || _interaction$request$ === void 0 ? void 0 : _interaction$request$[1]) || '',
        bodySize: responseBodyBuffer.length,
        content: {
          mimeType: interaction.response.headers['content-type'] || '',
          size: responseBodyText.length,
          text: responseBodyText,
          encoding: 'base64'
        }
      };
      let requestTiming = interaction.request.timingEvents;
      let startedDateTime = new Date(requestTiming.startTimestamp);
      yield {
        request,
        response,
        cache: {},
        timings: {
          send: 100,
          wait: 100,
          receive: 100
        },
        startedDateTime: startedDateTime.toISOString(),
        time: 100 + 100 + 100
      };
    }
  }
  static toHarJSON(entries) {
    const har = {
      log: {
        version: '1.3',
        creator: 'Optic capture command',
        entries: null // null will be dropped
      }
    };
    let tokens = async function* () {
      let entriesTokens = (0, _streamChain.chain)([_stream.Readable.from(entries), (0, _Disassembler.disassembler)()]);
      let manifestTokens = (0, _streamChain.chain)([_stream.Readable.from(async function* () {
        yield har;
      }()), (0, _Disassembler.disassembler)()]);
      for await (let token of manifestTokens) {
        if (token.name === 'keyValue' && token.value === 'entries') {
          yield {
            name: 'startArray'
          };
          yield* entriesTokens;
          yield {
            name: 'endArray'
          };
        } else if (token.name !== 'nullValue') {
          yield token;
        }
      }
    }();
    return _stream.Readable.from(tokens).pipe((0, _Stringer.stringer)());
  }
}

/*
  Optic was originally written to support base64 only in these fields. TBD why.
  If we want to work with HARs from other tools we should handle encoding at read-time, not write time

  Currently ok to continue this direction.
 */
exports.HarEntries = HarEntries;
async function toBase64(buffer, encodings) {
  try {
    const firstEncoding = Array.isArray(encodings) ? encodings[0] : encodings;
    if (Array.isArray(encodings) && encodings.length > 1) throw new Error('multiple content-encodinings are not supported');
    switch (firstEncoding) {
      case undefined:
        return buffer.toString('base64');
      case 'gzip':
        return await new Promise((resolve, reject) => {
          _nodeZlib.default.unzip(buffer, (error, result) => {
            if (error) {
              reject('could not decode ' + error.message);
            } else {
              resolve(result.toString('base64'));
            }
          });
        });
      default:
        throw new Error('unsupported encoding ' + firstEncoding);
    }
  } catch (e) {
    return Buffer.from('').toString('base64');
  }
}

// Mutate value by removing square brackets from IPv6 serverIpAddress entries.
// Square brackets are used to allow IPv6 to be used in URIs with (to distinguish
// from the delimeter for port). Chrome must have parsed them from the URI,
// not cleaning them up properly before including them in the HAR?
const ipv6BracketsPattern = /^\[(?<ip>.+)\]$/;
function coerceBracketedIpAddress(value) {
  if (value && typeof value.serverIPAddress === 'string') {
    var _value$serverIPAddres;
    value.serverIPAddress = ((_value$serverIPAddres = value.serverIPAddress.match(ipv6BracketsPattern)) === null || _value$serverIPAddres === void 0 ? void 0 : _value$serverIPAddres.groups.ip) || value.serverIPAddress;
  }
}
class HarEntryValidationError extends Error {
  constructor(invalidEntry, entryIndex, validationErrors) {
    const request = invalidEntry && invalidEntry.request;
    const url = request && request.url && (0, _isUrl.default)(request.url) ? request.url : null;
    const descriptions = validationErrors.map(error => `${error.instancePath} ${error.message}`);
    let message = `HAR entry #${entryIndex} not valid.\n${descriptions.map(description => `  - ${description}\n`)}`;
    if (url) {
      message += `\nRequest url: ${url}`;
    }
    super(message);
    this.errors = validationErrors;
  }
}
exports.HarEntryValidationError = HarEntryValidationError;